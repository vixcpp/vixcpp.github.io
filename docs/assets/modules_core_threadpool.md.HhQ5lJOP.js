import{_ as o,o as i,c as t,a}from"./app.BYybgkFJ.js";import"./chunks/markjs.l6TISjdC.js";import"./chunks/minisearch.DigTKvvb.js";const k=JSON.parse('{"title":"ThreadPool (vix::threadpool)","description":"","frontmatter":{},"headers":[],"relativePath":"modules/core/threadpool.md","filePath":"modules/core/threadpool.md","lastUpdated":1770805184000}'),r={name:"modules/core/threadpool.md"};function l(d,e,s,n,c,u){return i(),t("div",null,[...e[0]||(e[0]=[a('<h1 id="threadpool-vix-threadpool" tabindex="-1">ThreadPool (vix::threadpool) <a class="header-anchor" href="#threadpool-vix-threadpool" aria-label="Permalink to &quot;ThreadPool (vix::threadpool)&quot;">​</a></h1><p>This module provides a priority-based thread pool used by Vix for background work and heavy workloads.</p><p>Key features:</p><ul><li>Priority scheduling via a <code>std::priority_queue&lt;Task, ..., TaskCmp&gt;</code></li><li>FIFO behavior inside the same priority using a monotonic sequence number (<code>seq</code>)</li><li>Automatic worker growth when saturated (up to <code>maxThreads</code>)</li><li>Lightweight metrics: pending, active, timed out</li><li>Periodic scheduler threads that enqueue work at fixed intervals</li><li>Linux CPU affinity for worker threads (best-effort)</li></ul><h2 id="components" tabindex="-1">Components <a class="header-anchor" href="#components" aria-label="Permalink to &quot;Components&quot;">​</a></h2><h3 id="_1-task" tabindex="-1">1) Task <a class="header-anchor" href="#_1-task" aria-label="Permalink to &quot;1) Task&quot;">​</a></h3><p><code>Task</code> is the unit of work stored in the internal priority queue.</p><ul><li><code>func</code>: <code>std::function&lt;void()&gt;</code> executed by a worker</li><li><code>priority</code>: integer priority used by the queue</li><li><code>seq</code>: increasing number used to keep deterministic ordering</li></ul><p>Important detail:</p><ul><li>Higher <code>priority</code> runs first.</li><li>If two tasks have the same <code>priority</code>, smaller <code>seq</code> runs first (earlier enqueued first).</li></ul><h3 id="_2-taskcmp" tabindex="-1">2) TaskCmp <a class="header-anchor" href="#_2-taskcmp" aria-label="Permalink to &quot;2) TaskCmp&quot;">​</a></h3><p><code>TaskCmp</code> defines ordering for the priority queue.</p><ul><li>First compares <code>priority</code></li><li>Then compares <code>seq</code></li></ul><p>Because <code>std::priority_queue</code> keeps the &quot;largest&quot; element on top, the comparator is written so that:</p><ul><li>tasks with higher priority end up at the top</li><li>for equal priority, the smaller sequence number is served first</li></ul><h3 id="_3-taskguard" tabindex="-1">3) TaskGuard <a class="header-anchor" href="#_3-taskguard" aria-label="Permalink to &quot;3) TaskGuard&quot;">​</a></h3><p><code>TaskGuard</code> is a small RAII helper that increments a shared atomic counter on construction and decrements on destruction.</p><p>In this pool, it is used to track <code>activeTasks</code> safely even if the task throws.</p><h3 id="_4-threadpool" tabindex="-1">4) ThreadPool <a class="header-anchor" href="#_4-threadpool" aria-label="Permalink to &quot;4) ThreadPool&quot;">​</a></h3><p>The pool has two kinds of threads:</p><ol><li><p>Worker threads (<code>workers</code>)</p><ul><li>Wait for tasks</li><li>Pop the highest priority task</li><li>Execute it</li><li>Update <code>activeTasks</code></li><li>Notify <code>cvIdle</code> when the pool becomes idle</li></ul></li><li><p>Periodic scheduler threads (<code>periodicWorkers</code>)</p><ul><li>Sleep until the next tick</li><li>Enqueue a task into the main pool</li><li>Limit concurrency with <code>maxPeriodicThreads</code></li></ul></li></ol><h2 id="construction" tabindex="-1">Construction <a class="header-anchor" href="#construction" aria-label="Permalink to &quot;Construction&quot;">​</a></h2><p>Constructor:</p><ul><li><code>threadCount</code>: initial worker threads (clamped to at least 1)</li><li><code>maxThreadCount</code>: upper bound for auto-growth (clamped to at least 1)</li><li><code>priority</code>: default priority used by <code>enqueue()</code> overloads without an explicit priority</li><li><code>maxPeriodic</code>: maximum concurrent periodic scheduler threads (defaults to 4, clamped to at least 1)</li></ul><p>Behavior:</p><ul><li>Workers start immediately.</li><li>If worker creation fails, the pool stops and joins threads before rethrowing.</li></ul><h2 id="enqueueing-work" tabindex="-1">Enqueueing work <a class="header-anchor" href="#enqueueing-work" aria-label="Permalink to &quot;Enqueueing work&quot;">​</a></h2><p>There are three overloads:</p><ol><li><code>enqueue(priority, timeout, f, args...)</code></li><li><code>enqueue(priority, f, args...)</code></li><li><code>enqueue(f, args...)</code> (uses default pool priority)</li></ol><h3 id="timeout-behavior" tabindex="-1">Timeout behavior <a class="header-anchor" href="#timeout-behavior" aria-label="Permalink to &quot;Timeout behavior&quot;">​</a></h3><p>The timeout is for warning and metrics only.</p><ul><li>The pool does not cancel the work.</li><li>If the work finishes after <code>timeout</code>, a warning is logged and <code>tasksTimedOut</code> is incremented.</li></ul><h3 id="return-values" tabindex="-1">Return values <a class="header-anchor" href="#return-values" aria-label="Permalink to &quot;Return values&quot;">​</a></h3><p><code>enqueue</code> wraps the callable in a <code>std::packaged_task</code> and returns a <code>std::future&lt;ReturnType&gt;</code>.</p><p>This means:</p><ul><li>Exceptions thrown inside the callable propagate through the future.</li><li>You can <code>future.get()</code> to rethrow.</li></ul><h3 id="auto-scaling" tabindex="-1">Auto-scaling <a class="header-anchor" href="#auto-scaling" aria-label="Permalink to &quot;Auto-scaling&quot;">​</a></h3><p>On each enqueue, the pool checks:</p><ul><li><code>wcount = workers.size()</code></li><li><code>saturated = (activeTasks &gt;= wcount)</code></li><li><code>backlog = (tasks.size() &gt; wcount)</code></li></ul><p>If:</p><ul><li><code>wcount &lt; maxThreads</code> and <code>saturated</code> and <code>backlog</code></li></ul><p>then it creates a new worker thread.</p><p>This is a simple heuristic: it grows when all workers appear busy and the queue is building.</p><h2 id="periodic-scheduling" tabindex="-1">Periodic scheduling <a class="header-anchor" href="#periodic-scheduling" aria-label="Permalink to &quot;Periodic scheduling&quot;">​</a></h2><p><code>periodicTask(priority, func, interval)</code> creates a scheduler thread that:</p><ul><li>Ensures there are fewer than <code>maxPeriodicThreads</code> active periodic threads</li><li>On each tick, enqueues <code>func</code> into the main pool using <code>enqueue(priority, ...)</code></li><li>Warns if the posted task is not completed by the time the next tick occurs</li></ul><p>Important details:</p><ul><li>The scheduler itself does not run the job; it only posts it into the pool.</li><li>Periodic threads stop when <code>stopPeriodic</code> becomes true.</li></ul><h2 id="idle-waiting" tabindex="-1">Idle waiting <a class="header-anchor" href="#idle-waiting" aria-label="Permalink to &quot;Idle waiting&quot;">​</a></h2><p>Two helpers exist:</p><ul><li><code>isIdle()</code> returns true when there are no pending tasks and <code>activeTasks == 0</code></li><li><code>waitUntilIdle()</code> blocks until both conditions are true</li></ul><p>Implementation note:</p><ul><li>Workers notify <code>cvIdle</code> whenever they see both <code>tasks.empty()</code> and <code>activeTasks == 0</code> after finishing a task.</li></ul><h2 id="shutdown-and-lifetime" tabindex="-1">Shutdown and lifetime <a class="header-anchor" href="#shutdown-and-lifetime" aria-label="Permalink to &quot;Shutdown and lifetime&quot;">​</a></h2><p>The destructor stops everything and joins threads.</p><ul><li>Sets <code>stop = true</code> and <code>stopPeriodic = true</code></li><li>Wakes all waiters (<code>condition</code>, <code>cvPeriodic</code>)</li><li>Joins periodic workers then main workers</li></ul><p>Behavioral implications:</p><ul><li>Enqueue after stop triggers an exception (<code>ThreadPool is stopped; cannot enqueue new tasks</code>).</li><li>Stopping does not attempt to cancel running tasks; it waits for workers to exit after the queue drains.</li></ul><h2 id="platform-notes" tabindex="-1">Platform notes <a class="header-anchor" href="#platform-notes" aria-label="Permalink to &quot;Platform notes&quot;">​</a></h2><h3 id="linux-cpu-affinity" tabindex="-1">Linux CPU affinity <a class="header-anchor" href="#linux-cpu-affinity" aria-label="Permalink to &quot;Linux CPU affinity&quot;">​</a></h3><p>On Linux, worker threads attempt to pin themselves to a CPU core:</p><ul><li>Uses <code>pthread_setaffinity_np</code></li><li>Core index is <code>id % hardware_concurrency</code></li><li>If setting affinity fails, the pool logs a warning</li></ul><p>On non-Linux platforms, the affinity call is a no-op.</p><h3 id="windows-macros" tabindex="-1">Windows macros <a class="header-anchor" href="#windows-macros" aria-label="Permalink to &quot;Windows macros&quot;">​</a></h3><p>On Windows, the header temporarily undefines <code>min</code> and <code>max</code> macros to avoid conflicts with standard library headers.</p><h2 id="usage-example" tabindex="-1">Usage example <a class="header-anchor" href="#usage-example" aria-label="Permalink to &quot;Usage example&quot;">​</a></h2><p>Minimal usage pattern:</p><ol><li>Create a pool with a reasonable thread count.</li><li>Enqueue work with priorities.</li><li>Optionally wait for idle during shutdown.</li></ol><p>Example priorities convention (suggested):</p><ul><li>100: latency-sensitive tasks</li><li>50: normal tasks</li><li>10: background tasks</li></ul><h2 id="integration-note-for-vix-server" tabindex="-1">Integration note for Vix server <a class="header-anchor" href="#integration-note-for-vix-server" aria-label="Permalink to &quot;Integration note for Vix server&quot;">​</a></h2><p>In the HTTP pipeline, heavy routes are executed via an executor abstraction. The executor can be backed by this thread pool (or another implementation). A heavy route should:</p><ul><li>Avoid blocking the I/O threads</li><li>Run in the executor/pool</li><li>Return the response via the session write path</li></ul><p>That separation is what keeps the I/O threads responsive.</p><h2 id="quick-checklist" tabindex="-1">Quick checklist <a class="header-anchor" href="#quick-checklist" aria-label="Permalink to &quot;Quick checklist&quot;">​</a></h2><ul><li>Use a small <code>threadCount</code> for startup, allow growth with <code>maxThreads</code>.</li><li>Use priorities consistently across the codebase.</li><li>Use timeout warnings to find tasks that exceed your SLO.</li><li>Prefer short periodic tasks, and keep periodic concurrency bounded.</li><li>Call <code>waitUntilIdle()</code> only in controlled shutdown paths.</li></ul>',76)])])}const f=o(r,[["render",l]]);export{k as __pageData,f as default};
